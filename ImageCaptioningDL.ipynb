{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageCaptioningDL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1AD-X8TL-iFR7jb-lLoQjfQhftyR2MM4B",
      "authorship_tag": "ABX9TyOKitdm+GCo8iELhzLpNawd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mausumiR/deeplearning/blob/main/ImageCaptioningDL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFjX6pE7DqUd"
      },
      "source": [
        "# **cnn_rnn_assignment_set_8**\n",
        "\n",
        "## Group 28\n",
        "### Mausumi Ratha(2019ab04291)\n",
        "### Rabindra Kumar Panigrahi(2019ab04143)\n",
        "\n",
        "\n",
        "\n",
        "## Question:\n",
        "Image Captioning : Image Captioning is the process of generating textual description of an image. It uses \n",
        "both Natural Language Processing and Computer Vision to generate the captions. The dataset will be in the form [image \n",
        "â†’ captions]. The dataset consists of input images and their corresponding output captions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIt9aIcVfpry"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOipWihgDu9G"
      },
      "source": [
        "# **1.Import\tLibraries/Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYwuzgG9KXMs"
      },
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6EGRlc6gYKq"
      },
      "source": [
        "#Load Data\n",
        "url = '/content/drive/MyDrive/Colab Notebooks/set_2.pkl'\n",
        "with open(url, 'rb') as f:\n",
        "  caption_text = pickle.load(f)\n",
        "\n",
        "datatxt = []\n",
        "for text in caption_text :\n",
        "  for line in text.split('\\n'):\n",
        "    col = line.split('\\t')\n",
        "    if len(col) == 1:\n",
        "        continue\n",
        "    w = col[0].split(\"#\")\n",
        "    datatxt.append(w + [col[1].lower()])\n",
        "\n",
        "df_txt = pd.DataFrame(datatxt,columns=[\"filename\",\"index\",\"caption\"])\n",
        "uni_filenames = np.unique(df_txt.filename.values)\n",
        "print(\"The number of unique file names : {}\".format(len(uni_filenames)))\n",
        "print(\"The distribution of the number of captions for each image:\")\n",
        "Counter(Counter(df_txt.filename.values).values())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dVGb9NS_U92"
      },
      "source": [
        "### load the images\n",
        "dir_jpg = '/content/drive/MyDrive/Colab Notebooks/Flicker8k_Dataset'\n",
        "jpgs = os.listdir(dir_jpg)\n",
        "print(\"The number of jpg flies in Flicker8k_Dataset: {}\".format(len(jpgs)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gtZLek5Lwj7"
      },
      "source": [
        "# **2.Data\tVisualization\tand\taugmentation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vVqruCR-Mif"
      },
      "source": [
        "### a.\tPlot\tat\tleast\ttwo\tsamples\tand\ttheir captions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MVDPKjC96a2"
      },
      "source": [
        "\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "npic = 5\n",
        "npix = 224\n",
        "target_size = (npix,npix,3)\n",
        "\n",
        "count = 1\n",
        "fig = plt.figure(figsize=(10,20))\n",
        "for jpgfnm in uni_filenames[-npic:]:\n",
        "    filename = dir_jpg + '/' + jpgfnm\n",
        "    captions = list(df_txt[\"caption\"].loc[df_txt[\"filename\"]==jpgfnm].values)\n",
        "    image_load = load_img(filename, target_size=target_size)\n",
        "    \n",
        "    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n",
        "    ax.imshow(image_load)\n",
        "    count += 1\n",
        "    \n",
        "    ax = fig.add_subplot(npic,2,count)\n",
        "    plt.axis('off')\n",
        "    ax.plot()\n",
        "    ax.set_xlim(0,1)\n",
        "    ax.set_ylim(0,len(captions))\n",
        "    for i, caption in enumerate(captions):\n",
        "        ax.text(0,i,caption,fontsize=20)\n",
        "    count += 1\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2VY69P1BdiI"
      },
      "source": [
        "### b.\tBring\tthe\ttrain\tand\ttest\tdata\tin\tthe\trequired\tformat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkTiAa7zBzm9"
      },
      "source": [
        "## ***Text Preparation***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2D9LGSPVCHXI"
      },
      "source": [
        "def df_word(df_txt):\n",
        "    vocabulary = []\n",
        "    for txt in df_txt.caption.values:\n",
        "        vocabulary.extend(txt.split())\n",
        "    print('Vocabulary Size: %d' % len(set(vocabulary)))\n",
        "    ct = Counter(vocabulary)\n",
        "    dfword = pd.DataFrame({\"word\":ct.keys(),\"count\":ct.values()})\n",
        "    dfword = dfword.sort_values(\"count\",ascending=False)\n",
        "    dfword = dfword.reset_index()[[\"word\",\"count\"]]\n",
        "    return(dfword)\n",
        "dfword = df_word(df_txt)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpad1pakZ8HY"
      },
      "source": [
        " ##### Utility to clean the caption\n",
        "\n",
        "\n",
        "*   remove punctuation\n",
        "*   remove single character\n",
        "*   remove numeric\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb9CwQRpajZq"
      },
      "source": [
        "\n",
        "import string\n",
        "\n",
        "def remove_punctuation(text_original):\n",
        "    text_no_punctuation = text_original.translate(string.punctuation)\n",
        "    return(text_no_punctuation)\n",
        "\n",
        "\n",
        "def remove_single_character(text):\n",
        "    text_len_more_than1 = \"\"\n",
        "    for word in text.split():\n",
        "        if len(word) > 1:\n",
        "            text_len_more_than1 += \" \" + word\n",
        "    return(text_len_more_than1)\n",
        "\n",
        "\n",
        "def remove_numeric(text,printTF=False):\n",
        "    text_no_numeric = \"\"\n",
        "    for word in text.split():\n",
        "        isalpha = word.isalpha()\n",
        "        if printTF:\n",
        "            print(\"    {:10} : {:}\".format(word,isalpha))\n",
        "        if isalpha:\n",
        "            text_no_numeric += \" \" + word\n",
        "    return(text_no_numeric)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vqb6nGNMdFup"
      },
      "source": [
        "###### clean all captions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgF317CAdKMG"
      },
      "source": [
        "def text_clean(text_original):\n",
        "    text = remove_punctuation(text_original)\n",
        "    text = remove_single_character(text)\n",
        "    text = remove_numeric(text)\n",
        "    return(text)\n",
        "\n",
        "\n",
        "for i, caption in enumerate(df_txt.caption.values):\n",
        "    newcaption = text_clean(caption)\n",
        "    df_txt[\"caption\"].iloc[i] = newcaption"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MegIP4RNdddl"
      },
      "source": [
        "###### Add startseq and endseq tokens to captions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zms5Nh4tgCn5"
      },
      "source": [
        "from copy import copy\n",
        "def add_start_end_seq_token(captions):\n",
        "    caps = []\n",
        "    for txt in captions:\n",
        "        txt = 'startseq ' + txt + ' endseq'\n",
        "        caps.append(txt)\n",
        "    return(caps)\n",
        "df_txt0 = copy(df_txt)\n",
        "df_txt0[\"caption\"] = add_start_end_seq_token(df_txt[\"caption\"])\n",
        "df_txt0.head(5)\n",
        "del df_txt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5_cLgO0Sr5i"
      },
      "source": [
        "df_txt0.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEvtl5YMhfV2"
      },
      "source": [
        "## ***Image Preparation***\n",
        "### Extract Freatures of each image using Transfer Learning SqueezeNet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HNmsBIahnl1"
      },
      "source": [
        "\n",
        "# from keras_applications.imagenet_utils import _obtain_input_shape\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.layers import Input, Conv2D, Concatenate, \\\n",
        "     MaxPool2D, GlobalAvgPool2D, Activation, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import get_file\n",
        "#from tensorflow.keras.utils import layer_utils\n",
        "\n",
        "\n",
        "WEIGHTS_PATH = \"https://github.com/rcmalli/keras-squeezenet/releases/download/v1.0/squeezenet_weights_tf_dim_ordering_tf_kernels.h5\"\n",
        "WEIGHTS_PATH_NO_TOP = \"https://github.com/rcmalli/keras-squeezenet/releases/download/v1.0/squeezenet_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
        "\n",
        "def fire_block(x, squeeze_filters, expand_filters):\n",
        "    squeezed = Conv2D(filters=squeeze_filters,\n",
        "                      kernel_size=1,\n",
        "                      activation='relu')(x)\n",
        "    expanded_1x1 = Conv2D(filters=expand_filters,\n",
        "                        kernel_size=1,\n",
        "                        activation='relu')(squeezed)\n",
        "    expanded_3x3 = Conv2D(filters=expand_filters,\n",
        "                        kernel_size=3,\n",
        "                        padding='same',\n",
        "                        activation='relu')(squeezed)\n",
        "\n",
        "    output = Concatenate()([expanded_1x1, expanded_3x3])\n",
        "    return output\n",
        "\n",
        "# def SqueezeNet(include_top=True, weights='imagenet',\n",
        "#                input_tensor=None, input_shape=None,\n",
        "#                pooling=None,\n",
        "#                classes=1000):\n",
        " \n",
        "        \n",
        "#     if weights not in {'imagenet', None}:\n",
        "#         raise ValueError('The `weights` argument should be either '\n",
        "#                          '`None` (random initialization) or `imagenet` '\n",
        "#                          '(pre-training on ImageNet).')\n",
        "\n",
        "#     if weights == 'imagenet' and classes != 1000:\n",
        "#         raise ValueError('If using `weights` as imagenet with `include_top`'\n",
        "#                          ' as true, `classes` should be 1000')\n",
        "\n",
        "\n",
        "#     # input_shape = _obtain_input_shape(input_shape,\n",
        "#     #                                   default_size=227,\n",
        "#     #                                   min_size=48,\n",
        "#     #                                   data_format=K.image_data_format(),\n",
        "#     #                                   require_flatten=include_top)\n",
        "\n",
        "#     # if input_tensor is None:\n",
        "#     #     img_input = Input(shape=input_shape)\n",
        "#     # else:\n",
        "#     #     if not K.is_keras_tensor(input_tensor):\n",
        "#     #         img_input = Input(tensor=input_tensor, shape=input_shape)\n",
        "#     #     else:\n",
        "#     #         img_input = input_tensor\n",
        "\n",
        "\n",
        "#     input = Input([224, 224, 3])\n",
        "\n",
        "#     x = Conv2D(96, 7, strides=2, padding='same', activation='relu')(input)\n",
        "#     x = MaxPool2D(3, strides=2, padding='same')(x)\n",
        "\n",
        "\n",
        "#     x = fire_block(x, squeeze_filters=16, expand_filters=64)\n",
        "#     x = fire_block(x, squeeze_filters=16, expand_filters=64)\n",
        "#     x = fire_block(x, squeeze_filters=32, expand_filters=128)\n",
        "#     x = MaxPool2D(pool_size=3, strides=2, padding='same')(x)\n",
        "\n",
        "#     x = fire_block(x, squeeze_filters=32, expand_filters=128)\n",
        "#     x = fire_block(x, squeeze_filters=48, expand_filters=192)\n",
        "#     x = fire_block(x, squeeze_filters=48, expand_filters=192)\n",
        "#     x = fire_block(x, squeeze_filters=64, expand_filters=256)\n",
        "#     x = MaxPool2D(pool_size=3, strides=2, padding='same')(x)\n",
        "\n",
        "#     x = fire_block(x, squeeze_filters=64, expand_filters=256)\n",
        "    \n",
        "#     if include_top:\n",
        "#         # It's not obvious where to cut the network... \n",
        "#         # Could do the 8th or 9th layer... some work recommends cutting earlier layers.\n",
        "    \n",
        "#         x = Dropout(0.5, name='drop9')(x)\n",
        "#         x = Conv2D(filters=1000, kernel_size=1)(x)\n",
        "#         output = GlobalAvgPool2D()(x)\n",
        "#     else:\n",
        "#         if pooling == 'avg':\n",
        "#             output = GlobalAveragePooling2D()(x)\n",
        "#         elif pooling=='max':\n",
        "#             output = GlobalMaxPooling2D()(x)\n",
        "#         elif pooling==None:\n",
        "#             pass\n",
        "#         else:\n",
        "#             raise ValueError(\"Unknown argument for 'pooling'=\" + pooling)\n",
        "\n",
        "#     # Ensure that the model takes into account\n",
        "#     # any potential predecessors of `input_tensor`.\n",
        "#     # if input_tensor is not None:\n",
        "#     #     inputs = get_source_inputs(input_tensor)\n",
        "#     # else:\n",
        "#     #     inputs = img_input\n",
        "\n",
        "#     model = Model(input, x, name='squeezenet')\n",
        "\n",
        "#     # load weights\n",
        "#     if weights == 'imagenet':\n",
        "#         if include_top:\n",
        "#             weights_path = get_file('squeezenet_weights_tf_dim_ordering_tf_kernels.h5',\n",
        "#                                     WEIGHTS_PATH,\n",
        "#                                     cache_subdir='models')\n",
        "#         else:\n",
        "#             weights_path = get_file('squeezenet_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
        "#                                     WEIGHTS_PATH_NO_TOP,\n",
        "#                                     cache_subdir='models')\n",
        "            \n",
        "#         model.load_weights(weights_path)\n",
        "#         # if K.backend() == 'theano':\n",
        "#         #     layer_utils.convert_all_kernels_in_model(model)\n",
        "\n",
        "#         if K.image_data_format() == 'channels_first':\n",
        "\n",
        "#             if K.backend() == 'tensorflow':\n",
        "#                 warnings.warn('You are using the TensorFlow backend, yet you '\n",
        "#                               'are using the Theano '\n",
        "#                               'image data format convention '\n",
        "#                               '(`image_data_format=\"channels_first\"`). '\n",
        "#                               'For best performance, set '\n",
        "#                               '`image_data_format=\"channels_last\"` in '\n",
        "#                               'your Keras config '\n",
        "#                               'at ~/.keras/keras.json.')\n",
        "#     return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7WpF6dGonSB"
      },
      "source": [
        " \n",
        "input = Input([224, 224, 3])\n",
        " \n",
        "x = Conv2D(64, 3, strides=2, padding='same', activation='relu')(input)\n",
        "x = MaxPool2D(3, strides=2, padding='same')(x)\n",
        " \n",
        " \n",
        "x = fire_block(x, squeeze_filters=16, expand_filters=64)\n",
        "x = fire_block(x, squeeze_filters=16, expand_filters=64)\n",
        "x = fire_block(x, squeeze_filters=32, expand_filters=128)\n",
        "x = MaxPool2D(pool_size=3, strides=2, padding='same')(x)\n",
        " \n",
        "x = fire_block(x, squeeze_filters=32, expand_filters=128)\n",
        "x = fire_block(x, squeeze_filters=48, expand_filters=192)\n",
        "x = fire_block(x, squeeze_filters=48, expand_filters=192)\n",
        "x = fire_block(x, squeeze_filters=64, expand_filters=256)\n",
        "x = MaxPool2D(pool_size=3, strides=2, padding='same')(x)\n",
        " \n",
        "x = fire_block(x, squeeze_filters=64, expand_filters=256)\n",
        " \n",
        " \n",
        "x = Conv2D(filters=1000, kernel_size=1)(x)\n",
        "output = GlobalAvgPool2D()(x)\n",
        " \n",
        "#output = Activation('softmax')(x)\n",
        " \n",
        "from tensorflow.keras import Model\n",
        "model = Model(input, output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCsFjLx-Q5rX"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "weights_path = get_file('squeezenet_weights_tf_dim_ordering_tf_kernels.h5',\n",
        "                                     WEIGHTS_PATH,\n",
        "                                     cache_subdir='models')\n",
        "# initiate RMSprop optimizer\n",
        "opt = RMSprop(lr=0.0001, decay=1e-6)\n",
        "# Let's train the model using RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "model.load_weights(weights_path)\n",
        "loss_history = []\n",
        "model.summary()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-H3Jyo3nJK6"
      },
      "source": [
        "# Helper function to process images\n",
        "def preprocessing(img_path):\n",
        "    im = image.load_img(img_path, target_size=(224,224,3))\n",
        "    im = image.img_to_array(im)\n",
        "    im = np.expand_dims(im, axis=0)\n",
        "    return im"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRhc4xV1Y--9"
      },
      "source": [
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "images = OrderedDict()\n",
        "npix = 224\n",
        "target_size = (npix,npix,3)\n",
        "data = np.zeros((len(jpgs),npix,npix,3))\n",
        "\n",
        "for i,name in enumerate(jpgs):\n",
        "    \n",
        "    if i%1000==0:\n",
        "        print(i)\n",
        "    if(i>=3000):\n",
        "      break\n",
        "    # load an image from file\n",
        "    filename = dir_jpg + '/' + name\n",
        "    image = load_img(filename, target_size=target_size)\n",
        "    # convert the image pixels to a numpy array\n",
        "    image = img_to_array(image)\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    #nimage = preprocess_input(image)    \n",
        "    y_pred = model.predict(image).reshape()\n",
        "    images[name] = y_pred.flatten()\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAAosRCHTTq_"
      },
      "source": [
        "## **Link the text and image data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9n7DwKITIen"
      },
      "source": [
        "dimages, keepindex = [],[]\n",
        "descriptions = dict()\n",
        "#df_txt0 = df_txt0.loc[df_txt0[\"index\"].values == \"0\",: ]\n",
        "for i, fnm in enumerate(df_txt0.filename):\n",
        "    if fnm in images.keys():\n",
        "        dimages.append(images[fnm])\n",
        "        keepindex.append(i)\n",
        "        \n",
        "fnames = df_txt0[\"filename\"].iloc[keepindex].values\n",
        "\n",
        "\n",
        "\n",
        "dcaptions = df_txt0[\"caption\"].iloc[keepindex].values\n",
        "dimages = np.array(dimages)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ06xJcVTvLB"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "## the maximum number of words in dictionary\n",
        "nb_words = 8000\n",
        "tokenizer = Tokenizer(nb_words=nb_words)\n",
        "tokenizer.fit_on_texts(dcaptions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(\"vocabulary size : {}\".format(vocab_size))\n",
        "dtexts = tokenizer.texts_to_sequences(dcaptions)\n",
        "print(dtexts[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on_WYkZUT2s7"
      },
      "source": [
        "## **Split between training and testing data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ezge2_kT67u"
      },
      "source": [
        "prop_test, prop_val = 0.2, 0.2 \n",
        "\n",
        "N = len(dtexts)\n",
        "Ntest, Nval = int(N*prop_test), int(N*prop_val)\n",
        "\n",
        "def split_test_val_train(dtexts,Ntest,Nval):\n",
        "    return(dtexts[:Ntest], \n",
        "           dtexts[Ntest:Ntest+Nval],  \n",
        "           dtexts[Ntest+Nval:])\n",
        "\n",
        "dt_test,  dt_val, dt_train   = split_test_val_train(dtexts,Ntest,Nval)\n",
        "di_test,  di_val, di_train   = split_test_val_train(dimages,Ntest,Nval)\n",
        "fnm_test,fnm_val, fnm_train  = split_test_val_train(fnames,Ntest,Nval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SdmeaWiUBX1"
      },
      "source": [
        "maxlen = np.max([len(text) for text in dtexts])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWHGNalAUIch"
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def preprocessing(dtexts,dimages):\n",
        "    N = len(dtexts)\n",
        "    print(\"# captions/images = {}\".format(N))\n",
        "\n",
        "    assert(N==len(dimages))\n",
        "    Xtext, Ximage, ytext = [],[],[]\n",
        "    for text,image in zip(dtexts,dimages):\n",
        "\n",
        "        for i in range(1,len(text)):\n",
        "            in_text, out_text = text[:i], text[i]\n",
        "            in_text = pad_sequences([in_text],maxlen=maxlen).flatten()\n",
        "            out_text = to_categorical(out_text,num_classes = vocab_size)\n",
        "\n",
        "            Xtext.append(in_text)\n",
        "            Ximage.append(image)\n",
        "            ytext.append(out_text)\n",
        "\n",
        "    Xtext  = np.array(Xtext)\n",
        "    Ximage = np.array(Ximage)\n",
        "    ytext  = np.array(ytext)\n",
        "    print(\" {} {} {}\".format(Xtext.shape,Ximage.shape,ytext.shape))\n",
        "    return(Xtext,Ximage,ytext)\n",
        "\n",
        "\n",
        "Xtext_train, Ximage_train, ytext_train = preprocessing(dt_train,di_train)\n",
        "Xtext_val,   Ximage_val,   ytext_val   = preprocessing(dt_val,di_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd_lJCwqVPuF"
      },
      "source": [
        "from keras import layers\n",
        "print(vocab_size)\n",
        "## image feature\n",
        "\n",
        "dim_embedding = 64\n",
        "\n",
        "input_image = layers.Input(shape=(Ximage_train.shape[1],))\n",
        "fimage = layers.Dense(256,activation='relu',name=\"ImageFeature\")(input_image)\n",
        "## sequence model\n",
        "input_txt = layers.Input(shape=(maxlen,))\n",
        "ftxt = layers.Embedding(vocab_size,dim_embedding, mask_zero=True)(input_txt)\n",
        "ftxt = layers.LSTM(256,name=\"CaptionFeature\")(ftxt)\n",
        "## combined model for decoder\n",
        "decoder = layers.add([ftxt,fimage])\n",
        "decoder = layers.Dense(256,activation='relu')(decoder)\n",
        "output = layers.Dense(vocab_size,activation='softmax')(decoder)\n",
        "model = Model(inputs=[input_image, input_txt],outputs=output)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSnCGIULdXGs"
      },
      "source": [
        "# fit model\n",
        "import time\n",
        "start = time.time()\n",
        "hist = model.fit([Ximage_train, Xtext_train], ytext_train, \n",
        "                  epochs=5, verbose=2, \n",
        "                  batch_size=64,\n",
        "                  validation_data=([Ximage_val, Xtext_val], ytext_val))\n",
        "end = time.time()\n",
        "print(\"TIME TOOK {:3.2f}MIN\".format((end - start )/60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GtSQtO_dn0Z"
      },
      "source": [
        "print(Ximage_train.shape,Xtext_train.shape,ytext_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enDCkDY-dr5h"
      },
      "source": [
        "### Validation loss and training loss over epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13s8zuCedydT"
      },
      "source": [
        "for label in [\"loss\",\"val_loss\"]:\n",
        "    plt.plot(hist.history[label],label=label)\n",
        "plt.legend()\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i6CUTq3d3-I"
      },
      "source": [
        "## **Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBCy8r7Qd64a"
      },
      "source": [
        "index_word = dict([(index,word) for word, index in tokenizer.word_index.items()])\n",
        "def predict_caption(image):\n",
        "    '''\n",
        "    image.shape = (1,4462)\n",
        "    '''\n",
        "\n",
        "    in_text = 'startseq'\n",
        "\n",
        "    for iword in range(maxlen):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence],maxlen)\n",
        "        yhat = model.predict([image,sequence],verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        newword = index_word[yhat]\n",
        "        in_text += \" \" + newword\n",
        "        if newword == \"endseq\":\n",
        "            break\n",
        "    return(in_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK5kCV3nNKQZ"
      },
      "source": [
        "npic = 3\n",
        "npix = 224\n",
        "target_size = (npix,npix,3)\n",
        "\n",
        "count = 1\n",
        "fig = plt.figure(figsize=(10,20))\n",
        "for jpgfnm, image_feature in zip(fnm_test[:npic],di_test[:npic]):\n",
        "    ## images \n",
        "    filename = dir_jpg + '/' + jpgfnm\n",
        "   \n",
        "    image_load = load_img(filename, target_size=target_size)\n",
        "    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n",
        "    ax.imshow(image_load)\n",
        "    count += 1\n",
        "\n",
        "    # # captions\n",
        "    caption = predict_caption(image_feature.reshape(1,len(image_feature)))\n",
        "    ax = fig.add_subplot(npic,2,count)\n",
        "    plt.axis('off')\n",
        "    ax.plot()\n",
        "    ax.set_xlim(0,1)\n",
        "    ax.set_ylim(0,1)\n",
        "    ax.text(0,0.5,caption,fontsize=20)\n",
        "    count += 1\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJH1LGp3poqu"
      },
      "source": [
        "## **Bilingual evaluation understudy (BLEU)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOkXTwuapsN7"
      },
      "source": [
        "index_word = dict([(index,word) for word, index in tokenizer.word_index.items()])\n",
        "\n",
        "\n",
        "nkeep = 5\n",
        "pred_good, pred_bad, bleus = [], [], [] \n",
        "count = 0 \n",
        "for jpgfnm, image_feature, tokenized_text in zip(fnm_test,di_test,dt_test):\n",
        "    count += 1\n",
        "    if count % 200 == 0:\n",
        "        print(\"  {:4.2f}% is done..\".format(100*count/float(len(fnm_test))))\n",
        "    \n",
        "    caption_true = [ index_word[i] for i in tokenized_text ]     \n",
        "    caption_true = caption_true[1:-1] ## remove startreg, and endreg\n",
        "    ## captions\n",
        "    caption = predict_caption(image_feature.reshape(1,len(image_feature)))\n",
        "    caption = caption.split()\n",
        "    caption = caption[1:-1]## remove startreg, and endreg\n",
        "    \n",
        "    bleu = sentence_bleu([caption_true],caption)\n",
        "    bleus.append(bleu)\n",
        "    if bleu > 0.7 and len(pred_good) < nkeep:\n",
        "        pred_good.append((bleu,jpgfnm,caption_true,caption))\n",
        "    elif bleu < 0.3 and len(pred_bad) < nkeep:\n",
        "        pred_bad.append((bleu,jpgfnm,caption_true,caption))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl5MiA29pzkM"
      },
      "source": [
        "print(\"Mean BLEU {:4.3f}\".format(np.mean(bleus)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdGFfGmdp2Z6"
      },
      "source": [
        "def plot_images(pred_bad):\n",
        "    def create_str(caption_true):\n",
        "        strue = \"\"\n",
        "        for s in caption_true:\n",
        "            strue += \" \" + s\n",
        "        return(strue)\n",
        "    npix = 224\n",
        "    target_size = (npix,npix,3)    \n",
        "    count = 1\n",
        "    fig = plt.figure(figsize=(10,20))\n",
        "    npic = len(pred_bad)\n",
        "    for pb in pred_bad:\n",
        "        bleu,jpgfnm,caption_true,caption = pb\n",
        "        ## images \n",
        "        filename = dir_Flickr_jpg + '/' + jpgfnm\n",
        "        image_load = load_img(filename, target_size=target_size)\n",
        "        ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n",
        "        ax.imshow(image_load)\n",
        "        count += 1\n",
        "\n",
        "        caption_true = create_str(caption_true)\n",
        "        caption = create_str(caption)\n",
        "        \n",
        "        ax = fig.add_subplot(npic,2,count)\n",
        "        plt.axis('off')\n",
        "        ax.plot()\n",
        "        ax.set_xlim(0,1)\n",
        "        ax.set_ylim(0,1)\n",
        "        ax.text(0,0.7,\"true:\" + caption_true,fontsize=20)\n",
        "        ax.text(0,0.4,\"pred:\" + caption,fontsize=20)\n",
        "        ax.text(0,0.1,\"BLEU: {}\".format(bleu),fontsize=20)\n",
        "        count += 1\n",
        "    plt.show()\n",
        "\n",
        "print(\"Bad Caption\")\n",
        "plot_images(pred_bad)\n",
        "print(\"Good Caption\")\n",
        "plot_images(pred_good)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}